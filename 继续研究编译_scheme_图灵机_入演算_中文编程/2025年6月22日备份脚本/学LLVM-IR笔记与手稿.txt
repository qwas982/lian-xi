学LLVM-IR笔记与手稿


搜索这个网页{ https://getting-started-with-llvm-core-libraries-zh-cn.readthedocs.io/zh-cn/latest/ch06.html },逐行逐段地分析并给出{提纲与分支,
思维导图,
每个详细的知识节点,
把它们有机组合在一起输出一个思维链.}

//
没想到高速思考, 需要一个能瞬间生成的想象空间,
瞬间对思考对象其结构进行具象化,
思维的焦点或注意力, 需要聚焦在一种动态的流体上, 逻辑流.
这需要巨大的能量支撑, 需要生态环境提供的尽数能量种类.

每一个你怕的, 都是针对你不愿面对的最难的部分(们).
若想获取它, 几乎必须要尽数征服这些最难的点.

逐行解读,直接给出用中文伪代码表达{

//
怎样获得一种对计算机程序的黑客视角?
也就是把调试时、编译时、执行时它们的状态显示出来,
我假设有3栏,
1栏显示调试时, 调试器自动在源码行间跳转.
	这就像是一种静态分析, 调试的每一步每个过程都被实时地传给编译时.
2栏显示编译时, 源码字符串如何被分解与剖开, 变成牌序列、树、指令序列,
	在多个框里显示每个管道的输入与输出.
	自动画出控制流程图、有向无环图、支配树、各种块与操作.
	我想这需要建立多条管道行(流水线), 将其动画化, 从而显示其{流stream}.
3栏显示执行时, 在多个框里分别显示出指令序列、地址、寄存器状态、各种栈与帧, 比如操作数栈、调用栈等,
	各种堆, 比如数据堆、代码堆. 输入与输出的读写状态. 
更有说服力的动画: https://storage.googleapis.com/justine/sectorlisp2/sectorlisp2-final.mp4 ,
	我想这个动画只需要拓展就能展现我上面描述的各种视角.
	这应该比普通的调试要完善得多. 从头到尾地演示程序是怎么计算的?
	
哦, 这似乎还不够, 根据等式: 程序 = 算法 + 数据结构 , 
	加上现在有大模型的辅助, 我们似乎可以在调试时之前建一个新的动画,
	把我们的想法、手稿、任务、设计, 如何逐步地生成为算法, 如何生成结构,
	我们用自然语言描述这些内容, 大模型根据我们的描述生成对应的算法与结构之设计,
	最后大模型审核review这些代码, 人工排版, 或交给大模型排版,
	整理以后得到源码, 当然, 这部分的过程也需要显示出来, 就像动画一样, 
	我们该如何设计这样的图形显示器呢?
	
这确实是一套工程流程, 从创造程序, 到翻译程序, 到对程序求值得结果,
我们似乎完全可以将所有过程都可视化.
学llvm有感而发.


//
在大模型时代, 每个人都应熟练使用1~4个AGI大模型,
专业的工作人, 几乎必须要配备4个以上, 到8个甚至10个AGI大模型,
它们将是你十指的延伸, 也应是.

//
基块, 指令序列, 函数, 模块, 
控制流程图, 有向无环图, 支配树,
抽象句法树,
栈机器指令序列, forth程序, 直接线程码, 间接线程码,
遍历模式, 前序, 中序, 后序, 层序, 分别能得到不同特性的序列,
序列化, 反序列化,
操作, 结构, 算法结构, 数据结构,

微积分是一种思想, 我没有去学它的计算方法, 我把它当做一种思想来学.
微分为位, 积分为整.
微积分 = 演算.
既包括静态, 也包括动态.

//
设计一种使用栈机器指令序列风格的中间码-IR,
这次就不把它设计成字节码, 也不传给字节码解释器执行,
仅仅是当做这一种中间码来用, 力求简洁、简单, 易读易写易理解,
模仿LLVM-IR的设计, 但是不用它的简化指令集架构与三地址码形式的静态单赋值,
我要用栈机器指令风格 + 静态单赋值的形式来做中间码,
看了那么多种中间码-IR, 比如LLVM-IR, MIR, relay-IR, SRIR, CISC风格汇编, RISC风格汇编, 等等,
这些中间码几乎全都是寄存器机器的指令风格, 还没见到过栈机器风格的中间码.
用什么求值器来当执行器呢?
还得是forth, 实在是太简单了, 简单意味着快速高效出问题少.
遍历栈机IR, 生成forth的程序, 最后用forth的线程码虚拟机执行求值获得最终结果.
为了提高效率, 不以字节形式存储指令, 而是编码为二进制序列存储指令,
这可能需要学习LLVM的位码bitcode, 还有wasm的LEB128编码,
中后阶段解决了, 前阶段就不难了,
解析的问题, 就是剖开的问题, 就是线代与微积分的问题,
可以复用与递归, 使用中后阶段的算法与结构.


//
问了大模型才知道, 中间码技术真是太重要了.
既然可以用仿真技术编程并写出软件GPU, 那就可以做软件NPU, 这里面, 中间码技术有巨大的辅助作用, 因为中间码附带优化空间, 所以还能提升软件实现的xPU其性能.
把它们实现到一个最简图灵机的图灵完备指令集上, 如此一来就可以封装为一个接口, 物理硬件会随着时代的变化而变化, 但只要是图灵完备的, 就可以把这个接口映射到对应时代的物理硬件提供的指令集上. 同样的, 一些逻辑管道也可以编程为一种软件处理器.

提示语;
能否使用仿真技术, 把avx2编程为一个虚拟GPU, 游戏程序把它识别为GPU?
那我能不能用avx2模仿Intel Larrabee 的设计, 搞一种用avx2进行软件定义的 GPU?
图形 API（如 DirectX、OpenGL、Vulkan）这些不是有SPIR这种中间码么?
那我把avx2的指令集与寄存器封装为一个虚拟GPU, 对外暴露一套仿真的GPU指令集, 然后把SPIR翻译为这种指令集的指令序列不就行了?

//
最新实践经验; 所有重复的事, 都可以写脚本来解决.
脚本会自动帮你操作, 使用大模型辅助写脚本完全实现了发出自然语言到中文汉字编程的过程.

所谓LLVM基础设施, 换句话说就是一个内核, 模仿Linux内核的设计,
当然它还有大量的库帮助操作这个内核,

现实世界里, 大多数时候都是混合型的情况, 不可能非黑即白的.
没想到大模型的所谓"幻觉"竟然是类似人类想象力的东西,
不可思议, 要是优化掉了是多么的可惜.

原来根本就不需要组装多个垂直大模型去做一件事, 完全可以通过对算法、技术、方法的改进,
在一个大模型里就能做到, 这个行业更新真是太快了, 新东西新想法马上就可以得到验证,
从而促使你立即改变思路去重新设计.
例如;
{ 
搜索 { mamba + bitnet + MoE + CoT思维链 + LoT逻辑链 + 块序列 + 字节序列替换token }, 如何有机组合在一起, 能全部融合在一起.
; ---------------------------------------------
要将 **Mamba**、**BitNet**、**MoE（Mixture of Experts）**、**CoT（Chain of Thought）思维链**、**LoT（Logic of Thought）逻辑链**、**块序列** 和 **字节序列替换token** 有机地融合在一起，可以设计一个多模态、多层次的架构，充分利用每种技术的优势，同时解决它们的局限性。以下是一个可能的融合方案：

---

## 1. **架构设计：多层次融合模型**
### 1.1 **输入层：字节序列替换token**
- **目标**：直接处理字节序列，避免传统token分割的局限性。
- **实现**：使用 **BitNet** 的二进制编码技术，将输入数据（如文本、图像、音频）转换为字节序列，并通过 **Mamba** 的状态空间模型（SSM）进行高效建模。Mamba的线性时间复杂度和硬件感知设计非常适合处理长序列数据。
- **优势**：减少token化带来的信息损失，提升模型对原始数据的理解能力。

### 1.2 **特征提取层：块序列与Mamba**
- **目标**：高效提取全局和局部特征。
- **实现**：将字节序列划分为块序列（如固定长度的字节块），并通过 **Mamba** 的SSM进行特征提取。Mamba的选择性扫描机制可以动态调整信息流，捕捉长期依赖关系。
- **优势**：块序列化结合Mamba的线性复杂度，显著降低计算开销，同时保留全局上下文信息。

### 1.3 **推理层：CoT思维链与LoT逻辑链**
- **目标**：增强模型的推理能力和可解释性。
- **实现**：在推理任务中，引入 **CoT思维链** 和 **LoT逻辑链**。CoT通过显式生成中间推理步骤，提升复杂任务的解决能力；LoT则通过逻辑规则约束推理过程，确保输出的逻辑一致性。
- **优势**：结合CoT和LoT，模型既能展示推理过程，又能保证逻辑正确性，适用于数学推理、常识推理等任务。

### 1.4 **专家层：MoE（Mixture of Experts）**
- **目标**：动态选择最佳专家处理不同任务。
- **实现**：在模型的高层引入 **MoE** 架构，将多个专家（如Mamba模块、BitNet模块、CoT模块）组合在一起。MoE通过路由机制动态选择最相关的专家，提升模型的灵活性和效率。
- **优势**：MoE的稀疏激活特性大幅降低计算成本，同时允许模型根据任务需求动态调整专家组合。

### 1.5 **输出层：多模态生成**
- **目标**：生成多模态输出（如文本、图像、代码）。
- **实现**：结合 **Mamba** 的序列生成能力和 **BitNet** 的二进制解码技术，生成目标输出。对于复杂任务，可以结合CoT和LoT生成中间步骤，提升输出的准确性和可解释性。
- **优势**：多模态生成能力使模型适用于更广泛的应用场景，如文本生成、图像生成、代码生成等。

---

## 2. **技术融合的关键点**
### 2.1 **Mamba与BitNet的结合**
- **Mamba** 的线性时间复杂度和硬件感知设计非常适合处理 **BitNet** 的二进制编码数据。通过将Mamba的SSM与BitNet的二进制编码结合，可以高效处理长序列数据，同时减少计算开销。

### 2.2 **CoT与LoT的互补**
- **CoT** 通过显式生成中间推理步骤，提升模型的可解释性；**LoT** 则通过逻辑规则约束推理过程，确保输出的逻辑一致性。两者结合可以显著提升模型在复杂推理任务中的表现。

### 2.3 **MoE的动态路由**
- **MoE** 的稀疏激活特性允许模型动态选择最相关的专家（如Mamba模块、BitNet模块、CoT模块），提升模型的灵活性和效率。通过将MoE与Mamba、BitNet、CoT结合，可以构建一个高效且灵活的多任务模型。

### 2.4 **块序列与字节序列的优化**
- 将输入数据划分为块序列，并通过 **Mamba** 的SSM进行特征提取，可以显著降低计算开销，同时保留全局上下文信息。结合 **BitNet** 的二进制编码技术，可以进一步优化数据处理效率。

---

## 3. **应用场景**
### 3.1 **复杂推理任务**
- 结合 **CoT** 和 **LoT**，模型可以高效解决数学推理、常识推理等复杂任务，同时生成可解释的推理过程。

### 3.2 **多模态生成**
- 结合 **Mamba** 和 **BitNet**，模型可以生成文本、图像、代码等多模态输出，适用于更广泛的应用场景。

### 3.3 **高效长序列处理**
- 结合 **Mamba** 的线性时间复杂度和 **BitNet** 的二进制编码技术，模型可以高效处理长序列数据，如时间序列预测、长文本生成等。

---

## 4. **总结**
通过将 **Mamba**、**BitNet**、**MoE**、**CoT**、**LoT**、**块序列** 和 **字节序列替换token** 有机地融合在一起，可以构建一个高效、灵活且可解释的多模态模型。该模型在复杂推理任务、多模态生成和长序列处理等场景中具有显著优势，同时通过MoE的动态路由机制和CoT/LoT的推理能力，进一步提升模型的性能和可解释性。
}

//
使用中文编程 + 字符图重新表达逻辑链.

原来是农耕文明打不过工业文明,
我是从用大模型写Python脚本实现自动化去处理llvm大项目, 这个过程中悟到这一点.
农耕思维怎么可能打败工业思维,
工业文明是用机器操作机器, 农耕文明是用人操作机器,甚至用人操作人,
这根本就是有质的区别, 甚至不在同一个维度.
农耕文明唯一的优势, 也就是组织人群, 因为必须要把人组织起来, 才能实现大规模农耕,
产生农业, 手工业也是同理, 在这方面, 组织人可以说是农耕文明发展到登峰造极境的技术,
而工业文明的间接操作、管道, 这些东西对农耕来说是很陌生的,
难怪得打不过, 而我就处在农耕文明世界.
为什么近几十年, 这些人如此崇尚城市, 都想着往城市里钻, 仅仅是因为城市是工业文明的副产物,
这些人自以为是到了城市自己就是工业文明了, 不可思议,
而实际上发展几十年, 所得实践结果检验到的真理就是, 不过是把农耕文明那一套搬到了城市里,
这有何意义? 
完全变成了工业文明的活靶子!
我终于明白为啥有血汗工厂廉价劳动力了, 原来是原地踏步, 不思进取.
为啥农村包围城市成功了? 还是因为农耕文明的优势.
工业文明的最佳代表应该是德国工业机器思维, 难怪得派人去学德国,
但是也没把昂撒人的精髓学到.
腐儒这群儒教蠕虫果真是腐朽至极, 与螨虫狼狈为奸, 还发明什么中学为体西学为用,
所要干的事就是保留仕人吸劳动人的血, 不可思议的腌臜玩意儿.
现在不就是这样吗, 现实再次验证. 还将廉价劳动力称之为"人口红利", 简直是坏种透顶!
不会再996劳动交税供养这群不思进取的吸血鬼寄生虫.
原来秦政是想创造中国的原生工业文明. 所谓秦制, 还有排斥蠕虫的焚书坑儒.
秦制为啥天生跟封建礼教儒教蠕虫不兼容, 这些不正是指向秦政的宏图吗?
我明白我要做什么了, 
把农耕思维与工业思维, 合而为一,
原来先祖早就尝试过进步, 只可惜守旧势力太庞大, 孔老二呀孔老二, 坏种透顶!
终于被2000多年后的我重新发现,
学孔儒为啥不学道? 道才是原生本土哲学, 孔老二故意歪曲李耳的道学, 造出儒这种玩意儿,
为它的封建礼教服务, 我明白了.
我非常怀疑连山易和归藏易是被孔老二烧绝的, 为了实现孔老二的阴招, 用它自己造作的周易替代,
真正的易学从此失传,
不可思议呀不可思议, 孔老二真是用心险恶!
重新剖开秦政的秦制来瞧, 远远没有那么简单, 这真的是集权? 集中?
一种链式与树形的机构, 若是把这种机构递归地应用到农业与手工业呢?
不得了, 不得了, 这不就是工业的萌芽思想吗?
用机器操作机器 + 管道化.
原生数学-易学, 原生哲学-道, 原生民主-秦制, 原生工业文明-秦制,
我推测秦政是想用法家服务秦制, 没想到被法家反客为主, 那时代也不懂法家会产生啥后果,
秦政的军队为啥那么强? 不正是像工业机器一样推进吗, 只不过那时代的人不知工业为何物,
公输班机关术, 也是没有找到正主, 要不就早生几百年, 机关术也是一种法呀,
秦政未能找到机关术之法, 而是撞上了法家的法, 可悲可叹.
难以想象, 若是把易学 + 本土道 + 秦制 + 机关术, 组合到一起, 
我们的数学、哲学、民主、工业文明岂不是两千多年前就能达成?
只可惜历史不可假设,历史没有如果,
守旧势力一直都是很顽固, 包括现在都还是如此,
守旧势力阻碍进步.


//
我发现栈计算机竟然是先进的, 是80年代末90年代才出现的技术,
这比C要晚出现很久, 比lisp都先进,
也就是说lisp函数式编程能实现的, 栈计算编程也能实现.
SCIP-lisp.
LLVM-中间码IR.
栈计算机-forth. http://users.ece.cmu.edu/~koopman/stack_computers/index.html 
由此, 我获取到了从高级编程工具到底层硬件的全链路的计算理论指导,
我得到了简化编程的方法学.
ALU-原来是算法逻辑单元, 被误导了.
冯诺依曼结构, 图灵机, lambda演算, 原来要整合到一起去看.
未来必定是纯函数式编程, 或纯栈式编程的道路, C系C类果然是老态龙钟的待淘汰旧物.
前缀表达式到后缀表达式的翻译原来如此简单.
什么RISC或CISC, 原来都是过时落后的产物, 难怪编程如此复杂难用.
没想到栈计算的诞生处竟然是关联到医疗相关的, 这指向了什么?

//
编程语言可能真的是红海了,
大模型越来越强, 以后哪还需要通过编程语言获取程序产品啊,
试想下, 要是大模型以后能直接生成AST或中间码呢?
这不就是个内置编译器的编程大师吗,
现在的编译器开发人员都是人形自走的编译器, 比如rust开发人员都是人肉编译器, 那换个思路, 如果大模型把这套流程学会了呢?
o3的强逻辑推理就能达到博士级别, 难以想象以后的大模型会强到什么程度.
现在的机器学习神经网络也是自带token化工具,
还有两点没有攻破, 1是BNF文法, 2是机器执行程序进行求值.
如果大模型把冯诺依曼结构 + 图灵机 + lambda演算三者的本质学到,
那还有BNF和处理机什么事?

现在的思维链大模型以后的逻辑链大模型, 都是自带逻辑思维,
人类的提示语, 模糊的提问, 大模型会自动用逻辑思维进行解码, 从而秒秒钟把人类的模糊提问变化为精确的逻辑伪代码, 而伪代码到程序代码的翻译那就太简单了, (ΩДΩ)震惊... 原来李艳红与皮衣黄的断言是真的呀.

我敢断定, 人类已创造的所有知识, 包括整个软件工程体系(系统, 编译器, 图形计算, 网络, 数据库等等), 甚至包括各个细分领域的行业(比如医疗, 教育, 工业, 农业等等), 如果全部用程序代码来表达与保存, 不会超过1万亿行!
因为人类的这些所谓的细分行业内, 大量的知识都是重复的, 从软件工程体系为各行各业做计算机辅助编程就可看出, 并且, 即使是软件工程体系本身内在的代码都包含大量重复, 1万亿行, 128TB的硬盘, 可以装下所有! 还有相当大的冗余空间!
甚至把大模型本身(GB级) + 人类DNA(GB级)都可以装下.

并且, 从最近deepseek对大模型的优化来看, 要达到GPT4的智能水平, 从万亿级形参量(1.8T)减少到千亿级形参量(671B)了, 推测得知, 也就是说未来大模型的智能水平会在递减的形参量上获得递增的智商! 千亿级到百亿级到十亿级, 甚至亿级百万级, 就能达到GPT4智商的大模型还有几个月就会诞生呢? 
同理可证, 如果按照这样的比例律`scale law`发展, 再把形参量给加到万亿级, 那大模型的智商岂不超越人类了吗? 
而现在的计算卡也越来越强, 很快就会有单张卡跑一个1万亿形参量大模型的计算能力了吧. 那如果是计算卡集群呢? 难以置信. (ΩДΩ) 我觉得未来的大模型智商会强得可怕!

大模型写代码造程序生产软件产品, 这是生产力呀, 一旦突破, 那岂不就可以把这种生产力经验应用到别的行业? 比如做3D模型图, CAD图, 这些都是高度精确的, 再扩展一下, 把生产力应用到工业农业其他行业呢?
不可思议, 也就是说, 一旦突破了自动编程, 人类所有生产力行业都会完全实现自动化.
再配上3D打印机? 好家伙, 3D模型直接翻译为现实世界!
若把这样的技术应用于web3.0呢? 数字孪生元宇宙虚拟现实看来还真不是开玩笑.
你能想到的, 人类能想到的, 大模型迟早都会想到, 人类再也没有秘密咯,
不管是人体还是人脑, 对大模型来说都不再是黑盒, 对人类自己来说也不是黑盒了.
感觉像三体人似的,
我大胆预言, 透明时代, 即将来临.
科技爆炸, 几乎是必然发生. }

笔记;
经过实践, 我发现最先考虑的应该是实现一个虚拟机,
随后实现这个虚拟机的汇编器,
中间码就是机器无关汇编,
之后就可以用这套汇编来写程序了,
但是在实现虚拟机之前, 需要先设计指令集.
这时, 选择什么样的句法, 就需要有相应指令形式支持,
而什么样的句法涉及到文法BNF的设计.

//
中国为什么不做编译器和编程语言？
https://www.zhihu.com/question/21449634

因为太复杂了,

业界的文档、代码, 包括释经权都是为那一套(ALGOL, FORTRAN, Pascal, 这类编程风格及其子孙)复杂的生态服务的,

最近rust圣战又重复了一遍, 昂撒人乐此不疲.

我认为未来即便又出某种新编程语言, 昂撒人还是会继续玩弄.

反观我国从业者, 都在为生计发愁, 思考的也是怎么依靠资本, 给资本打工来求生.

所谓搞开源呢, 不过是一层遮羞布, 根本不是真正的开源, 是为资本服务的开源.

斯托曼搞GNU可不是基于这样的信念和理想搞开源.

我国从业者在求生与开源之间没找到新道路, 甚至根本都没有去找.

这里存在一个自举的问题; 怎么从开源里生存起来, 而不是依赖资本.

; ----------------------------------

要做自己的基础设施生态, 首先起点要高, 不能去重复昂撒人做过的那些事,

比如做栈机: Stack Computers： the new wave ,

可以看下这本书, 这是比CISC与RISC更先进更高级的体系结构, 并且栈机还简单, 我说的这个简单是全方位的, 硬件电路可以比RISC更简单, 编程也比CISC与RISC更简单.

简单就意味着做编译器更快, 是个人稍微学习点基础知识就可以做, 我完全不保留这些所知所想, 相反, 我希望国内更多的从业者以及入门小白或门外人都参与这些新东西, 这意味着可以似雨后春笋一样壮大新生态.

编译器其实是汇编器, 汇编器的本质就是编译器, 可以看下LLVM的工程源码,

LLVM-IR, 我称之为: 底层虚机-中间码, 它的本质就是宿主在一个汇编器界面里,

所谓模块、函数、基块、指令序列, 就是一套规范化的机器无关汇编设计结构, 它是一个树结构, 句法树或者有向无环图表示, 直观显示出控制流程图(你需要打印出来).

中间码的三种表示形式, 一种是内存里运行的AST, 一种是像普通汇编代码的序列(字节码, 助记符), 一种是机器码形式的二进制序列(重点, 是二进制整数, 不是"字节码"), 就这么多了, 其实底层虚机-中间码就这么点东西, 其它的都是cpp工程问题, 全英文儿编程, 从其项目结构与标识符命名方式可以窥之.

而wasm是一种对底层虚机-中间码的简化, wasm我称之为: 网汇, 网页汇编嘛.

底层虚机-中间码是基于RISC设计的, 而网汇则是基于栈机设计的.

栈机不管是基于硬件还是基于软件, 都是西方软件工程体系发展到后期的产物, 成熟产品是80年代末90年代才出现的, 前面也说了, 不管是硬件还是软件的实现都很简单, 可以看出昂撒人也喜欢简化, 从CISC到RISC就是一种简化, 所以我们应该站在巨人的肩膀上.

简化不但可以省晶体管省电路, 意味着降低成本, 还可以省脑力省人力, 意味着可以扩大规模扩大生产. 昂撒人不傻滴, 反观国内早期从业者却不是这样, 以复杂为荣, 甚至延续到现在.

所有这些现象可以解答, 为啥我们做不出编译器, 设计不出划时代的编程语言(语言? 错! 工具? 对).

;---------------------

我给出的建议是; 一切从简单开始, 也不要去关注性能如何, 低又咋样? 就算是一秒钟执行一条指令操作又如何? 先跑起来才是正解, 未来的硬件远超你在当下的纠结, 根本就不是问题之所在! 栈机对应的有栈编程工具-forth族系, 其实这就是一种高阶汇编, 我们完全可以把它当一种中间码, 在这之上设计更简单更符合人类语言习惯的编程工具! 你想要的数学编程, 自然语言编程, 全都不在话下, 若是要基于CISC与ALGOL及其子孙去设计数学编程自然语言编程, 那就... ... 可能走入歧途了吧, 点错科技树.

; -------------------------------------------

另外,

我认为未来应该是基于MIMD多指令多数据编程, 皮衣黄用SIMT验证了平行(并行)的可能型与强大. 所以为了满足未来生产需求, 比如AGI大模型, 比例律scale law需要一种疯狂的并行计算, MIMD必然是下一个攻克的城池. 假如量子计算机迟迟不能发展出来, 量子计算编程生态现在还是零. 还全是实验室产物, 既未商用量产, 也未出现消费电子化趋势.

那可能需要一种比栈更强的结构, 啥呢? 我认为是链表. 因为只有链表才能实现MIMD, 它的指令(们)用一个链表装下, 它的操作范围也用一个甚至多个链表装下, 在一周期内计算完SISD或SIMD需要多个周期才能计算完的数据量. 其电路的设计也是一个挑战, 不过挑战意味着机遇和潜力和下一个风口.

这样的机器编程应该比栈机更加简化, 我预测, 这种编程应该是比脚本(例如AppleScript)还要简单, 因为底层都实现了链表, 那上层呢?

我也是最近才知道所谓结构其实就是类型, 而算法也是一种结构, 所以本质问题是类型, 而主义(-ism), 其实也是一种类型, 所以世界的本质问题是---型 . 你也可以称之为模或范. 我说的是汉字, 不是编程术语哈.

不过这些还是捕风捉影的东西,

还是一切从简单开始吧.

//
原来底层世界, 一切都是[符到数]的翻译,
人能看懂字符, 10进制 16 8 2进制的整数数目, 
但不方便直接看数目, 所以要看字符,
而机器方便读写与操作整数数目, 其中2进制是直接操作,
10进制 16 8进制还需要再翻译一遍,
而图像视频音频也是编码为整数再翻译为2进制,
人看懂字,机器看懂数,
中间不过是一对一的翻译,
看不见摸不着的抽象内涵=>熵, 就宿主在两者内.

原来翻译过程如此简单, 符到数, 不就是键值对吗,
翻译的重要方法, 趟, 使用多趟的方式简化步骤,
每一趟都扫描键序列, 也可以称之为遍历,
每趟为一个特定目的扫描, 文本字符串描述的程序有多个字段, 每次扫描特定字段,
所获得的键值序列存储到类似的结构里, 比如哈希表,
专业的术语是符号表,
经过多趟扫描后, 获得全部程序信息,
翻译规则预先存储在符号表,
比如什么字符对应什么二进制,
还有行号用于定位符号, 为生成地址占位.


//
怎样设计一个提示语?
大模型偏好结构化提问, 
明确的指令式计算机,
结合冯氏结构, 图灵机, lambda演算,
格式;
{
    目的:
    输入:
    输出:
    操作:
        数据:
        步骤:
        元循环求值器:
}

根据结构化提问, 实现需求:
{
    目的: 开发一个汇编器,
    输入: 字符串, 数字,
    输出: 二进制序列,
    操作: 按逻辑需求有序执行,
        数据: 
            cpp23, 
            松耦合高内聚, 
            RALL内存管理, 
            `use 命名空间 std`, 必须达到1千行代码,
            设计一套图灵完备指令集, 基于栈机器的指令风格,
        步骤: 
            首先写出整体架构的伪代码,
            根据伪代码填充实际的cpp代码,
            多趟遍历, 每趟仅完成特定种类功能,                        
            设计基于哈希表的符号表,
            读取输入的汇编文件,
            写入输出的二进制文件,            
}

对于结构化提示, 读取=>理解=>分析=>求值=>生成.
思维链辅助理解, 模块->函数->基块->指令序列.
学习冯氏结构, 图灵机, lambda演算等等计算理论.
{
    功能名称:
        栈机仿真,
    参数:
        程序计数器,
        栈 { 1栈, 2栈, }
        指令集 { 推, 弹, 移,
                加, 减, 乘, 除, 取模, 幂, 与, 或, 非, 异或, 左移, 右移, 递增, 递减,
                分支, 跳, 零跳, 非零跳, 环,
                测试, 对比, 等于, 不等, 大等, 小等, 大于, 小于,
                叫, 返,
                入, 出, 中断, 空操作, 停机, }
        内存 { 程序内存, 指令内存, 数据内存, }
    步骤:
        从文件内读取二进制序列,
        初始化一个栈机需要的全部零件,
        初始化状态码集合,
        初始化异常控制,
        在一个状态机里实现{ 拾取->译码->执行->写回 }操作管道,
        把结果写入一个缓冲区,
    返回值:
        打印缓冲区的值,
}
使用多趟来完成,
第一趟实现伪代码,
第二趟实现松耦合高内聚的各模块,
继续趟,
每趟只实现一个简单的目的.

; ----------------------------

经过实践测试,
我发现基于思维链的大模型确实可以做到一些最基本的开发流程了,
关键字; 与大模型交互、伪代码、中文编程、多趟、迭代
而传统模型根本没有这种思考能力,
思维链大模型是正确发展方向, 这才第一代, 远远不够,
还需要纯逻辑链的大模型, 加上对多模态与三维时空的理解,
要给这种大模型配上摄像头与机械臂,
这样它就能经过思考再做事, 
按部就班地做事, 学习微积分的思维一样做事, 分治, 把大问题分解为小问题序列, 还复杂就继续分解,
也不可能像现在的传统大模型一样一口气就想着把全部事情做完,
怎么可能嘛!
即使是人类也办不到呀,
未来还是充满潜力, 基于思维链发展出逻辑链大模型, 到纯逻辑链, 形成致密逻辑网,
这样就可以实现大模型自动编程, 从细碎的小步骤开发直到完成一个大项目,


//
web3.0需要什么样的基础设施?
我读了js的历史才发现, 原来web2.0时代的一切东西都跑在一个仿真层上.
包括js本身, 浏览器, HTML css, 之外的Python, LLVM-IR, 最近的wasm, 等等,
所有这些web2.0的东西, 包括多媒体, 都是都跑在仿真层上,
C与古代cpp原来早就是落后被抛弃的产物, 而js是用现代cpp实现的,
而rust这种倒退的产物还能在这个时代出现并兴盛, 实在是匪夷所思,
现代cpp确实要简单好多倍, 若全用最新cpp标准编程, 现代cpp几乎跟Python和js一样简单,
我似乎理解了为啥cpp可以编译为wasm机器码.
但是这些都是为web2.0服务的产物.
web3.0时代该怎么办?
虚拟现实,元宇宙,大模型,数字孪生,图形计算,工业4.0,等等,这些东西确实强大,但又很巨大,
我从[栈机器:新浪潮]里得到启发,
硬件;使用MIMD实现高度并行计算, 软件:使用栈语言forth当中间码IR,
在这之上, 要设计一种可以编译到栈语言中间码的新编程工具.
它不但要更加地简单, 还要满足web2.0时代的需求, 更重要的; 要能承载web3.0的一切事物,
这种语言必然是要为高度并行计算优化,
让我看看,
第一, 几乎必须是函数式, 因为函数式编程天生契合并行与平行,
第二, 要基于更高级的数据结构, 比如栈, 链表, 图, 等等,这些在底层就要实现,
第三, 简化易用, 要支持数学编程与自然语言编程, 所以Unicode几乎是必须, 甚至还应创造一种更强的编码标准,
第四, 要原生支持仿真计算, 在web3.0时代这就像空气与水一样, 几乎是必须的,
第五, 为原生并行编程设计, 那c类传统的串行编程就显得非常不适合,
第六, 要有原生指针类型, 若在句法层面就指定了, 那么在实现层面就会完善,


创造语言, 第一步应该是制定标准,
之后按照标准设计BNF, 有了BNF就可以被解析器解析.
读取器=>扫描器=>牌器=>解析器(输入牌根据BNF)=>抽象句法树=>中间码(机器无关汇编),
符号表也是很重要, 首先要设计存储AST的数据结构, 之后遍历AST进行语义分析,
同时生成符号表与中间码, 符号表记录作用域和生成地址, 地址很重要, 后面一连串步骤都要用到地址,
之后通过中间码的汇编器翻译到具体的系统与硬件平台的指令序列,
这其中还需要链接器与加载器,
当然, 中间码也可以直接执行, 这就是虚拟化技术的虚拟机,
虚拟机也可以直接执行字节形式的中间码, 或编码为二进制的机器码, 全凭预先设计定调,
而栈语言forth程序可以直接执行, 用不着这么多步骤.
栈语言forth程序可以当做中间码使用, 无疑会大大简化后端的设计.

原来UTF8是一种二进制, 它根本不应该称之为"编码", 真是极大误导.


//
经历十多二十年后, 我才明白一个事实,
我的处境所在环境, 根本没有资源时间让我连续地学习,
我只有碎片化的、分散的学习机会,
我终于明白我为什么学不会编程,
我只能掌握一些布局规划的、理论设计的技能,
可悲的不切实际.

所谓面向对象, 就是在高阶函数+闭包的基础上,
把数据结构与它的操作函数通过松耦合紧内聚封装并缝合到一起,
信息时代的石油是什么?
我认为是编程、程序、代码,

//
提示语;

学习`.cpp`的标识符翻译风格, 进行字面直译, 使用中文编程方式去翻译代码清单, 注意翻译时命名不要与py已有关键字或内置功能重名.
注意识别英文里的*大小写* *单复数* *缩写简写* *带有划线*区别和习惯等等, 在中文里没有对应表示所以要特别给你说.
你可以在翻译的标识符后加上 `_大写` `_复` `_缩 _简` 下划线+单位进行修饰, 学习下, 举一反三以此类推.


逐行解释代码, 并在行后用中文注释其功能, 注意是每行后!

//
我好像明白了一点, 所以GPU从硬件到软件这一套软件栈, 是一种类型1 hypervisor的虚拟化, 
GPU+显存 : 硬件, 
驱动 : 一个系统内核, 
图形函数库+部分驱动功能 : 编译器, 
游戏程序 : 上层应用.

也就是说, 仅仅实现指令集虚拟化是不够的, 
还必须要有一个内核来管理资源.

整个GPU组件链是对传统的处理器+操作系统的递归级打包,
也就是说,
就算是要实现硬件仿真,
也要跟着实现这套仿真硬件的配套工具链.

通过对GPU系统地学习,
我似乎知道了怎么利用类型1 hypervisor来实现仿真NPU与仿真CPU.
以及各种按需设计的xPU.

若是把各种处理器都按照仿真来设计, 做出一个仿真层,
那么, 可能需要一种全新的硬件来跑这种程序链软件,
我把它称之为全功能处理器,
它必须是并行的, 所以MIMD指令集类型必须要得到支持,
还需要为其设计一种新型的编程工具,
forth类语言也是最新的探索, 为了契合全功能处理器+多指令多数据指令集,
可能需要设计一种栈+链表的高级语言, 类似uiua, apl, 
或者链表里嵌入数组,
把这种语言翻译为forth类语言的程序,
本质上来说, forth程序其实是一种高级汇编,
而汇编分为; 机器相关汇编, 机器无关汇编,
后者等于中间码,
在这个基础上又发展出了高级、中级、低级中间码,
这应用了分治与微分思想, 进行逐级迭代, 多趟完成, 来化繁为简, 便于使用.


//
不去研究那些咯, 
不去研究CISC, RISC了,
LLVM-IR也是个RISC副产物,
要实现计算, 怎么就这么难,
只研究wasm网汇这样的类型了,
以wasm作为切入口,
研究栈机, 
硬件栈机没条件研究,
只能通过仿真的方式研究栈机了,
好在早已有模板可供参考,
the Novix NC4016 栈机器, forth编程语言,
前者对应wasm的二进制格式, 后者对应wasm的文本格式wat.
原来forth就是nc4016的汇编.
现在要做的就是,
给栈机器栈语言也做一套像wasm这样的仿真计算层.
想要使用计算机来实现计算, 明明可以很简单的,
一群串行脑壳非得把计算的过程整得那么难.
再也不用使用那套屎山了.

Harris Semiconductor RTX 2000
Harris Semiconductor RTX 32P


//
我发现伪代码路线好像没什么卵用,
新的大模型技术会改进那些不完善的瑕疵,
用伪代码来帮助编程, 就相当于是一个代数数据类型加模式匹配,
是非常表面、肤浅的皮毛, 是很快就会过时的为了应对旧大模型瑕疵的缝缝补补手段,
新的不断改进大模型本体的技术层出不穷,
未来大模型必然学会递归自改进, 这样看的话, 
现在这些应对瑕疵的想象设计, 简直不值一提.

未来可能只需要一个架构师模型, 一个开发者模型,
两个模型互相配合, 各管各的, 松耦合高内聚, 和谐共生,
而人类只需要与模型们交流与沟通即可, 


//
原来仿真之后, 是逆向工程.
最开始我想要的原来是这个, 但那时我完全理解不了.
这真是一环扣一环,
逆向→仿真→中间码→计算原理→编程→lisp→forth→算法.
缺一点不可, 缺一点你又完全不懂其它, 就少了衔接,
正如逆向的本质, 追踪逻辑过程, 跟踪线索, 复制_模仿痕迹.

我似乎找到了forth真正的价值在哪,
线型流!
它要比一般的汇编码、中间码更易读、易理解、易用.

而lisp也是如此地重要,
树形结构, 极为方便人类使用,
因为人的认知, 人的脑子, 都是树形结构的, 
这其实是某种天然的契合,
好多领域都要用到树形结构, 它便于人类读取和理解.
lisp可以通过代码或编程的方式, 直接映射为利于人类的某种本质结构关系.
这种翻译过程很重要.

序列到结构, 结构到序列, 还有它们的操作.
这就是为啥需要算法, 为啥需要抽象为数据.

人认识自然对象, 改造自然对象.
这或许就是人类的工具思维, 其强大之处吧.

//
cuda是个编译器, 把C/CPP代码编译为PTX中间码.
cuda还提供一些库文件, 就像libc 或 glibc一样.

想要替代cuda可太简单了,
只要重新写个类似的编译器即可.

PTX是一种中间码,
跟LLVM-IR, SPIRV是一样的,
这类中间码也可以称之为"机器无关汇编",
我最近在研究中间码,
这个东西一看就懂.

中间码的优势就是可以优化指令序列,
还有就是可以极为方便地翻译为其它架构的汇编,不管是机器相关, 还是机器无关.
只要写个翻译器, 就可以把PTX翻译为AMD gpu可以执行的汇编, 或升腾加速卡、或摩尔线程gpu可执行的汇编.

当然, 我们也可以自创一套跟PTX一样的中间码, 用来替代PTX.
假如英伟达连PTX也禁用的话,(因为PTX这也是它这个私有公司的私有资产

欢迎大家一起来研究中间码, 这个东西其实简单得很,
但是换来的利益结果、计算价值、效率的提升却是平方级的.