从上到下;    

 大模型    
    ↓    
 Python    
    ↓    
 pytorch    
    ↓    
   CPP    
    ↓    
   CUDA    
    ↓    
   汇编    
    ↓    
  机器码    
  
; ----------------------------    
  
大模型: 基于数学的数理逻辑(各种方程与函数)构造的模型,    

Python: 使用py脚本调用pytorch的操作,    

pytorch: 把对机器学习的各种操作封装为各种操作库,    

CPP: 实现机器学习库,    

CUDA: cpp调用CUDA的并行计算库,    

汇编: cpp + cuda的源码编译为PTX中间码与SASS汇编,    

机器码: 汇编翻译为芯片能直接执行的机器码.    

; --------------------------------    

在这个纵轴上也有横轴;    

比如大模型就有卷积网、递归网、变形器网之分,    
除了py以外, 还可以用其它编程语言当脚本使用, 例如mojo,    
除了pytorch以外, 还有TensorFlow,    
除了cpp,还可用mojo, 或别的编程语言实现,    
除了CUDA还有ROCm、oneAPI、CANN,    
汇编除了PTX与SASS以外, 还有HSAIL:GCNISA,L0IR:GenISA,    
机器码层面, 除了英伟达GPU以外, 还有其它厂商的GPU、NPU, 当然也可用CPU.    

所以这是个软件工程体系, 是个产业链.    

; ------------------------------------    

所以, 这套组合结构的本质是什么?    

我认为是[并行计算], 模拟平行可能型.    
模拟、仿真、虚拟化,    
高超虚-hypervisor,    

那, 如何才能把这个过程简化?    
已经有了0到1, 现在需要从1到0,    
松耦合高内聚的优势是降低模块之间的副作用,    
首先要创造一种多指令多数据的指令集,    
分成三段式设计; 前端、中端、后端,    
使用微积分思维进行分治与递归,    
使用慎独维持熵减,    

; -------------------------------------    

还有个问题是数学编程与改变计算模式,    

当前无法实现真并行, 即使是使用了多指令多数据, 也还是伪并行的范畴,    
在这样的情况下, 就必须要使用虚拟化技术来实现隔离与封装,    
让程序在一个虚拟化的环境内计算,    
使用仿真处理器, 模拟一个真并行的环境,    
在外部看来是伪并行, 在内部看来是真并行,    
例如[时间分片、协作式多任务处理、抢占式调度、协程、状态机、事件驱动编程]等技术,    
让运行在虚拟机上的程序认为自己处于一个真并行环境中,    
运行在虚拟机上的程序无需感知底层的实际执行模型, 它们会认为自己处于一个真正的多线程或多核环境中.    

为了实现数学编程,    
我选择在函数式编程上实现编译工具链, 我认为lisp与forth都是函数式编程,    
抽象后就是树结构与栈结构,    
主编程语言在句法上使用树结构lisp,    
演示;    
```
(defun 解析二次方程 (a b c)
  "解析二次方程 ax^2 + bx + c 并生成 Forth 代码。"
  `(lambda (x)
     (+ (* ,a (expt x 2))
        (* ,b x)
        ,c)))

;; 示例用法：
(let ((方程 (解析二次方程 2 3 1)))
  (print 方程))  ; 输出: (lambda (X) (+ (* 2 (EXPT X 2)) (* 3 X) 1))
```

中间码使用栈机的栈式风格,    
演示;    
```
二次方程:
    DUP          ; 复制 x
    EXP 2        ; 计算 x^2
    PUSH 2       ; 把 2 推入栈中
    MUL          ; 计算 2 * x^2
    PUSH 3       ; 把 3 推入栈中
    MUL          ; 计算 3 * x
    ADD          ; 把 2 * x^2 和 3 * x 相加
    PUSH 1       ; 把 1 推入栈中
    ADD          ; 把结果与 1 相加

```

机器汇编使用栈结构forth,    
演示;    
```
: 二次方程 ( x -- 结果 )
    dup 
    2 ** 2 *       # 计算 2*x^2
    3 * +          # 加上 3*x
    1 +            # 加上 1
;
```


在树结构之上, 还需要一种脚本, 一样使用栈结构的句法,    
但它更偏向于人类可读的自然语言句法,    
人类使用这种脚本按照数学方程与函数的数理逻辑思维来书写程序,    
从而实现数学编程,    
演示代码;    
```
: 二次方程 ( a b c x -- 结果 )
    # 计算 ax^2 + bx + c 的值
    a 乘以 x 平方 加上
    b 乘以 x 加上
    c
;
```

```
: 正弦波 ( 振幅 频率 相位 时间 -- 结果 )
    # 计算正弦波：振幅 × sin(2π × 频率 × 时间 + 相位)
    振幅 乘以
    2 圆周率 乘以 频率 乘以 时间 乘以
    相位 加上
    正弦
;
```

当然, 还可以在这之上继续抽象, 比如把图编译为这种脚本,    
人类看图比看字要更有效率.    


现在不是有代理agent吗, 国内称之为智能体, 炸不讨论这个代理编程呢? 大模型自动编程,     我发现这些人的认知水平非常低下, 大模型只是个神经网络, 就像你碳基脑一样, 你能一次型记住一个项目,     为一个项目建立逻辑树记忆树什么的吗? 很显然不行!!!    

代理agent, 记住了!  国外的游标+克劳德3.7的编程能力, 大家有目共睹, 还不说这才年初, 年中,     年末再来看代理编程的能力! MCP协议就是为了完善上下文关系型数据的,     在MCP基础上建一个上下文关系型数据库又有多难? AST DAG CFG, 大模型可以通过MCP调用编译器工具链生成啊!     用LLVM生成简直易如反掌. 现在的大模型又掌握了思维链, 最近是真的在研究思维树了, 未来思维图 思维网什么的,     发展出来只是时间问题. 并且, 我还看到一个用编程语言压缩逻辑思维的, 再用自然语言解压缩的研究,     还是国内大学出的论文, 年初还有谷歌的泰坦架构新模型论文, 我难以想象, 要是把这些都整合到一起,     超越克劳德3.7只是时间问题! 在这样的大模型上再做出更强的代理呢? MCP协议是否还有改进空间?     加上互联网主干线网关级路由协议, MCPv2够不够强, 一切都还要发展. 这样的编程能力出现后, 我很难说,     大模型自动编程 + 深度研究, 是否可以获得递归自改进, 大模型自主自动改进自身的组成部分.    

自动学习 = 强化学习 + 思维链 + 推理时计算(也叫测试时计算), 记住唠!           昨天我还问了大模型是否有其它神经网络技术, 一问才知道除了ANN还有SNN, 被称为尖钉神经网, 国内翻译为脉冲神经网. 据说是第三代神经网. 现在用的变形器架构是继RNN CNN之后的, 是ANN的一个变种分支,     尖钉网是完全模仿碳基脑的思考方式, 试想下, 这些已经存在的神经网技术都各有缺陷,     但是如果未来的大模型具备了递归自改进, 自动学习 + 深度研究, 它们把这些神经网络技术长处优点都整合到一起,     再自动编程实现, 那会如何? AGI不成问题吧.    

人工神经网络（ANN）      
├── 前馈型网络 → FNN → RBF      
├── 循环型网络 → RNN → LSTM → Tree-LSTM      
├── 卷积型网络 → CNN      
├── 生成对抗型网络 → GAN      
├── 图结构型网络 → GNN → GCN      
├── 注意力机制型网络 → Transformer      
├── 脉冲神经网络 → SNN      
└── 自编码器与能量模型 → AE → BM/RBM      

; -------

我潦草看了下这些文章, 这个人研究大模型似乎连大模型的存在基础都还不懂,     

大模型本身是数字神经网, 从逻辑层面模拟碳基脑的工作方式, 字符文本音频图片首先被转化为牌, 牌又被转化为数字, 这个数字被称为向量! 并且, 现在还处于技术发展初期, 但人类已经没太多能力给它添砖加瓦,     所以需要大模型具备[自动编程] 与 [自动研究]的能力, 让它自己研究自己, 要做到这点(也就是人类给出的引导程序), 还需要一两个算法的突破, 比如"长期记忆力", "思维链在向量空间里运行", 现在的思维链是在牌上推理的,     这显然是计算量暴增导致效率低下, 强化学习+思维链(思维树_思维图_思维网),     可以被认为是一种可以改进长期记忆力的候选技术之一, 在控制向量的工程能力上再进步些,     比如7B就可以造出数学能力登顶的大模型, 依此类推, 数学好, 其它各方面也不差, 那么7B就还有巨大改进空间,     所需的就是向量计算的工程问题, 深求已经通过开源周告诉了我们应该怎么研究这些工程问题. 1是向量的结构,     2是向量的效率, 之后才是控制与计算, 现在大模型的编程能力已然是码农水平, 虽然离专家级还有些距离, 但是生成一个函数_一个类已经问题不大, 根据中间码层级; 模块 函数 基块 指令, 的分类来看,     大模型除了模块级还无法胜任, 其它已经可以了, 于是, 我们人类研究人员可以利用这种编程能力, 重写机器学习库,     比如CUDA cpp, pytorch cpp等等, 改造计算库, 使其更适合大模型的运行, 更开放和简单,     全开源社区都可以参与去改善各个细节, Python脚本只是调用这些库, 所以改进的核心在于机器学习库,     机器学习库是操纵向量的本体所在, 而向量是神经网的权重存在的基石(起码现在的变形器注意力架构是如此),     所以研究人员只要设计算法, 函数以下的代码交给大模型去自动生成就好, 之后再用大模型进行审查review,     这与过去相比已经是大大改善开发速度和负担了, 可以预见的是, 一旦大模型掌握了模块级生成能力,     基本上离大模型自己写代码改进自身, 也就不远了, 又因为一个7B的MoE在数学能力上就可以达到陶哲轩的层次, 所以大模型设计算法也是不在话下, 我们人类只要告诉大模型我们的思路即可, 大模型可以用思维链整理我们给出的思路, 之后翻译为算法, 再把算法翻译为模块级代码, 当然, 一定要专门训练这样的垂直模型,     即-擅长编程的大模型(数学_通用_逻辑_代码), 比如克劳德3.7(国内似乎不重视编程能力大模型的训练),      模块级代码就像乐高积木一样, 就像拼图, 或者我们汉文化的木工活铁匠活,     我们只需要把这些组件组装起来就能做出完整的成品. 以上我说的这些, 大模型也是可以学习的.    

那个人的文章才是真正的全凭想象, 全是些唯心主义形而上的东西, 他还想研究大模型的心智, 无语了,     人类连自己的心智_意识这些都还没搞清楚, 那人显然没有科学思维, 他不是凭实验_试验获取认知, 而是全凭他臆测!     论文水得再多, 没有做实验, 怎么就断定出结果_结论了!    

; --------------------------

强化学习RL通过增加隐藏层获得了认知能力的提升, 从4层到1024层的变化, 是可见的质变,     https://arxiv.org/abs/2503.14858 , https://github.com/wang-kevin3290/scaling-crl ,     在这个基础上配合MoE并增加到1万层, 叠加思维链(思维树_思维图_思维网)向量化, 牌向量化,     并且调制成多模态(音频_视频_图像), 通过MCP调用函数与工具(API),     这样的大模型再用编程语言在向量空间里压缩逻辑思维, 之后用自然语言解压缩后表达,     我认为其编程能力是可以达到专家级的, 并且因为压缩的关系, 可能整个模型并不大, 在3B 1.5B上即可实现.     
大模型是如何解决问题的? 把一个问题分解为多个小问题, 这不就是分治算法吗? 这是一种算法思维,     大模型可以通过思维链学会掌握, 通过MoE在某些细分知识分支上达到擅长与熟练的层次,     通过增加隐藏层的层数获得长期记忆力, 不在牌token上做思维链推理, 而在向量空间里进行思维链,     提升了效率降低计算量, 多模态意味着可以像人一样看懂屏幕上的内容, 听懂声音里的语义, 通过MCP调用外部对象,     即-使用API的能力, 就像是给大模型安装了四肢与皮肤, 获得了新的感知方法, 使用编程语言压缩逻辑思维,     可以高效理解与表达, 几句话就能表达完不至于长篇大论锚不中靶.     
分解大问题为数个小问题, 再递归这个过程, 小问题分解为小小问题, 不断逐步细分下去, 直到细粒度达到原子,     这样子问题就会极为简单, 这个过程不就是使用树形结构么, 我能不能看作是化为树的过程?     之后再用思维链去遍历每层树的节点, 逐个解决问题节点, 随后思维链再把已解决的结果串联起来, 归纳总结得出结论, 好家伙, 像什么? 不就是微积分? 分解问题是微分的过程, 获取答案是积分的过程, 在这样的思路上推理,     这一切都在强化学习上工作, 在向量空间里计算, 数千层数万层的隐藏层+MoE正好为这个理论提供容纳空间, 不可思议, 要是这个理论能落地被应用, 我认为实现AGI就是近在眼前了.    

; -----

可以再添加大模型写CUDA cpp(不过最好选取适用于国内加速卡的CANN cpp, MUSA cpp)核函数算子库能力的测例,    

另外,    
对于大模型编程能力来说,    
还需多用强化学习在多种编程语言的AST、CFG、DAG、支配树等等语法无关的结构上做大规模纵向训练.     提升生成复杂函数、闭包、类的能力, 目前先写好一个个模块为目标.    
而这关系到大模型对内存模型的理解, 不管是项目的内存模型还是语言本身的内存模型.    
学形式逻辑, 学语义, 才能进步, 要不一换编程语言, 生成代码的性能就下降.    
这是第一步, 在语法无关的结构上学形式逻辑、学语义    

第二个是, 编程大模型需增加并强化多模态视觉能力,    
能看到IDE或编辑器的UI界面, 这很重要, 我们人类也不是闭眼编程的, 现在大模型既看不到也听不到.    

第三个是MCP, 这是一种大模型与外界通信的管道, 类似网线或总线bus, 你也可以想象成大模型的手脚与皮肤,     即-物理感知, 现在大模型是"输出"token的方式在写代码, 而不是像人一样敲键盘, 也就是说,     大模型需要通过MCP规范I/O的能力.    

一点总结, 希望国内大模型厂商能重视这三点.    

并且, 现在大模型的配套技术也越来越完善了,    

汇总一下找到的解决方案 = [    
"只要增加专家数与隐藏层, 这种纵向的深度, 模型性能就能得到持续大幅提升",    
"强化学习, 思维链, 多模态思维链",    
"在向量空间里推理, 而不是在token上推理",    
"还发现MCP是一个通信管道总线, 用于连接各种不局限为同一种类型的节点" ]    

若是年中或年末就能实现专家级编程, 那样的话, 以后4核8G跑个AGI是不是也能行?    

期待.    

; -----

可以推测出,    

未来的完全体模型应该是; 曼巴Mamba + 变形器Transformer + 稳定扩散Stable Diffusion =     抽象出一个综合型的新技术.    
通过整合各自的长处优点, 来补全当前大模型的部分缺陷.    
我想到一个疑点,    
为什么大模型不使用栈来装当前推理_思考的结果呢?    
大模型在向量空间里思考, 得出结果后放到栈里, 接着可以清空注意力, 去思考下一步呀,    
而栈里的结果向量就被牌化器Tokenizer牌化为具体的字符_文本_图像_音频等等形式.    
输出完了后, 当前栈就是空的, 就不占地方了,     
如此一来, 大模型岂不就具备了无限长度的上下文空间?    
哦, 还有对齐数据, 这可以再增加一个栈来存储, 不用存放具体的向量, 而是高维向量,    
换句话说, 就是对齐数据与之前注意力的关系,    
大模型通过读取这个栈里的状态获取当前处于上下文里的啥位置, 且不局限于文本向量,    
从而不需要极为冗长的牌token上下文长度,    
也就是, 把向量空间里的上下文与牌上下文分开对待, 松耦合解绑.    

; ------

在一次偶然打开计算器作四则运算时, 我无意中处在"程序员"模式,    
我输入的时候突然领悟到, 这不就是实时翻译么?    
我输入的是十进制数值, 在十进制上操作. 但与此同时, 十六进制与二进制与八进制也在实时变化,    
关键点是二进制的立即变化引起我的顿悟! 试想下,     
这时候内存里的数据与处理器里的数据是不是也是这样即时与上下文对应变动的?     
而计算器显式地向我展示了内存与处理器内部的状态,    
引申推理下,    
若扩展这四种类型的示例呢?    
想象下, 增加字符串类型, 增加跳转类型(流程控制), 这不就是完备程序的组成部分了吗?    
字符串也是经过多次翻译, 比如十进制到十六进制, 最终翻译为二进制的,    
只不过这个在计算器上没有部署也没有显式存在.    
也就是说, 若是有这部分功能, 那么, 当我输入字符的时候,     
字符在内存与处理器内的状态也会实时映射到计算器上显式展示,    
这是一个很好的窥探图灵机计算原理的窗口, 计算器上已经自带算术逻辑操作,    
即-ALU算法逻辑单元, 而控制器与存储器是隐式存在的,    
人在使用计算器时, 其实就是充当程序实现中转操作, 外部的问题是输入,    
人+计算器+操作, 实现计算理论的计算过程.    

; ------------------

返回一个翻译后的源码{    
1,使用你最擅长的编程语言翻译.    
2,在翻译过程中, 使用中文标识符.    
3,代码业务逻辑不可改变.    
4,使用AST、CFG、DAG、支配树等抽象能力强化理解代码的形式逻辑和语义.    
5,使用OOP组织代码结构.    
6,使用数据类型优化性能.    
}读取要求, 以生成高质量且准确、松耦合高内聚的复杂函数、闭包、类为主.    

; ---

【颠覆传统】从 Lambda 演算到 Lean 4，再到编译原理与逆向工程：一条通往编程本质的学习路径,    

没想到函数式还真的是银弹,    
函数式是能通达自然语言、伪代码、数学、编程的中心岔口.    
我似乎找到了完美的伪代码编写器.    
一种真·中间语言, 像桥一样的存在, 过渡区.    
"用S表达式精准给出`.py`文件内全部代码的AST, S表达式的标识符用中文."

词法分析器 × , 字法分析 √ , 输入字, 输出词素_语素, 扫描器 → 牌器.    
若想支持中文汉字汉语, 只需选对字符集, 比如Unicode, 以此作字库奠基设计基础.    

语法分析器 × , 句法分析 √ , 输入词素_语素, 输出带有递归性质的节点链, BNF、CST、AST、结构、符号表.    
中文编程在这一步其实已经无关紧要了, 因为词素_语素可以编码为数目序列, 句法分析的是一串串数值,    
所谓生成代码_指令, 实质是访问节点链, 打印每个详细的操作,    
操作形式_方式跟具体的芯片所属指令集架构强相关, 也可以是一台虚拟_仿真_模拟的机器.    

其它函数式、栈式, 都可以丢掉了, 瘦肉4_lean4真是一个集大成者,     
什么haskell、APL、UIUA, 扔了扔了,    

请问你是不是可以用`lean4`表达每个输出token, 而不是局限于LaTeX + Markdown这一种样式?    
Lean 4可以用中文汉字汉语来表达吗,    
好的, 演示一个来瞧瞧,    
这个Lean 4怎么看起来跟编程语言有些类似? 它是编程语言吗? 又觉得跟lisp有些相似.    
使用瘦肉4实现这3个样例,    
这么看的话, 我是不是可以丢掉lisp_Scheme了, 直接学lean瘦肉4进行编程不更强更直接吗. 可以回答那个怎么自学编程的知乎问题了, 从lambda演算 → lisp_Scheme → lean瘦肉4, 再附加编译器_编译原理 → 中间码 → 逆向.    

不是不用那些现成数据,    
而是预训练数据集不准确, 即使是36万亿数据集, 内部也是各种自相矛盾、充满悖论的情况, 这就是使用人类标注的缺陷, 而当前又无法从物理世界获得物理反馈数据(信息).    
大模型训练了这些数据, 就会产生强烈的幻觉, 导致在输出牌token、推理的时候, 出现各种编造, 要不就偏离, 或者不信自己, 本来第一步就对了, 后面越推理越错.    

另外, 希望深求把形式化验证数据集预先翻译为中文汉字汉语, 让大模型用中文做默认思考语言, 把华夏文化的内在逻辑内化到其思考源头上, 汉人积累了5千年的错误经验, 现在应该继承到大模型里去.    

; ---------


对于"无实体抽象"来说, 可能与大部分人、大部分场景都有关系吧.    

; ---

通过数学公式、定理 + 程序代码, 压缩逻辑,    
之后在文本、多模态上, 对齐这个压缩后的逻辑,    
压缩逻辑 = 压缩智慧,    
继续在向量空间内强化学习这些压缩智慧, 降低幻觉,    
如此一来, V4就可以不再需要六千多亿参数量也可实现相等的性能, 比如说从千亿降到百亿甚至几十亿,    
或者反过来, 继续增加参数量到万亿甚至几万亿, 不管是文本还是多模态, 全都给他弄上, 于是变成世界模型,    
不过想要实现这个还有点距离, 还没有原生训练对代理agent的使用, 还没训练对外界环境交互的强化学习, 甚至还没研发一个比MCP/A2A更先进的通信能力, 类比人类或其它生物的五感六觉一样的传感器, 大模型在这方面还是零, 想要达到世界模型目前还太难.    

不知这样推理对不对.    

或许应该从形式化验证、离散数学、计算机科学等角度, 创造出全能的架构师程序员, 数学作理论指导, 编写代码解决实际问题, 不局限于某一种编程语言的语法, 而是练内功, 这样就能获得专家级编程大模型, 随后, 再用这个模型去自动编写新的大模型框架, 而研究员们, 只需要专注于改进模型、算法、设计本体上即可获得越来越强的大模型.    

预训练, 包括预训练数据集, 预训练权重, 这些都是固定的, 导致强化学习得出的效果_性能, 都不佳.    
并且, 数据集是导致各种幻觉的元凶!    
大模型必须要转变架构, 通过MCP_A2A协议的通信管道, 与外界环境交互, 从环境与交互里获取实时的物理反馈数据与信息, 并即时更新权重, 这样一来, 就能实现即时学习新事物, 并立即应用新学到的知识技能.    

人类碳基脑参数量才86B, 20瓦功耗, 不但有智能还有意识, 而现在千亿万亿参数量大模型, 百万瓦功耗, 既没有意识也没有智能, 也就是说, 现在的大模型发展方向99%大错特错, 99%不能实现AGI.    

//    
掌握逻辑思维就行了, 逻辑思维 = 算法思维,     
算法 = ["结构"、"步骤"、"操作"、"指令"、"变量"],     
记住这个数组, 它们是树形关系. 遇到问题或内容,     
瞬间想象一个结构(类似网、块、框), 把遇到的情况里,     
几个关键点_重点作为变量存储到结构的节点上,     
把它们连线, 之后再想象有多少种组合,     
你测试每种步骤的时候, 就是在操作组合, 不断循环往复,     
久而久之, 你会总结出几大类人类常用的逻辑链,     
你可以给它们命名, 也就是设计一个指令, 要用的时候,     
随时根据指令名检索到. 人类社会常用的逻辑链,     
数量有限, 不要以为很多, 掌握了算法思维,     
你可以用算法思维实现复制型学习, 再来看课本内容 或     
社会内容, 你会发现简单至极,     
不管啥东西你都能瞬间抽象出重点_本质在哪.                                                                                                                                            
             具体学什么不要问我, 内容要根据自己兴趣去找.    
             
//    
回到真空, 从零到一.    
我是从独立思想悟出,     
独立于家庭, 独立于社会, 独立于国家, 独立于地球,    
独立于太阳系, 独立于银河系, 独立于宇宙.    

函数式,    

宇宙缩放,    

//    
[舍弃CUDA编程！CMU等用几十行代码将LLM编译成巨型内核，推理延迟可降6.7倍]    

果然不出我所料, 运行大模型的基础设施, 还有巨大改进空间.     
不管是软件层面还是硬件层面.    

//    
为什么现在的人形机器人愚傻 呆滞 恐怖谷 不堪大用?    
因为它的设计方向是错的!    
造机器人 ×    
造义体 √    
参考攻壳机动队, 比如草薙素子, 它全身义体, 只有她的脑子是她的, 碳基神经网, 也就是一个缸中之脑,     
通过全身义体连接世界.    
所以造人形机器人这个赛道, 一开始就走错了发展方向!    
应该造全身各部分义体, 最后再组装起来, 硅基神经网宿主在芯片上.    
