我是头一次用,
在虚机里用, 因为我不想被疯狂读写SSD杀固态的写入寿命.
使用感受是:
性能太差!

我建议, 立即跳出js/ts开发环境.
用cpp或铁锈实现, 且就用trae本身去实现.

第二,
不宜搞出两个功能上有重叠的主体, 建造器/solo.
"慎独",
"若无必要, 勿增实体",
既然是在开源版VScode上裁剪, 仅应该保留其核心, 例如好用的编辑器.
你们的trae核心, 应该用别的实现.

我使用的时候, 屡次碰到崩溃, 导致任务中断.
说下界面设计;
从友好、专业、简洁, 之角度, 说下我的设想.
不知你们是否见过"可视化的机器运行状态", 它是一种比调试界面,
还要更极客的视觉.

CPU状态, 栈区(多个), 堆区(多个), 被完全可视化.
用16进制, 实时显示各区块的运行时状态.
(当然, 你用10或2进制也可以, 未来可以用拓扑几何或电磁波)
你能想象吗?
就像我用win的计算器_程序员页, 当我输入数目后,
其状态, 在16、10、8、2进制之间, 实时转换与显示.

我描述这些的目的是, 
告诉你们去设计一种面向未来的, 与AI协作并交互的界面.
不要恪守在过去的茧房里.

当我打开trae时, 我可以只看到一个悬浮窗,
从悬浮窗, 可以进入一个REPL, 支持富文本, 多媒体.
也就是说, 不要固化在文件管理器 编辑器 chat三栏上.
它们可以作为标签, 在我需要的时候弹出.
在内部, 可以作为一个大模型的函数/工具调用.
我在REPL里, 跟大模型对话, 大模型拥有开启与关闭窗口的权限,
当然, 也可以设计出手动开关的按钮.
在REPL里, 我说: 显示状态图,
大模型会把当前项目的流程图/依赖图_CFG/DAG/支配树, 显示出来, 
并把状态指针, 指向当前步状态.
在REPL里, 我说: 跳到xx模块,
大模型会立即显示xx模块位于状态图的某位置,
并调用编辑器, 跳到指定代码行位置.
在REPL里, 我说: 我要看代码地图,
大模型会立即把一个项目的全部函数显示为各种各样的"盒子",
把数据显示为"矿",
把"用-定"/"定-用"显示为它们之间的连线. 你可以想象成道路/江河.
在REPL里, 我说: 我们来review,
大模型会根据架构图生成文档,
并征求我, 使用何种排版便于我查阅.
在REPL里, 我说: 调试与测试,
大模型立即准备一个完全图形化的独立调试界面,
并加载可视化运行时, 同时写出测试代码, 从外部影响调试进程,
为我(开发者), 实时显示程序在编译时 运行时 加载/卸载时的关联状态.
等等.

我知道实现这些, 在过去仅靠人力的情况下, 是一种天方夜谭.
但现在是AI编程时代, 不可再用过去的茧房视角看待.

GUI一定要保留, 不可终端化.
GUI一定要用cpp/c/ruts实现, 
不要再用js/ts v8.
VScode, 只是一个被调用的工具模块, 不可当做主要核心.

AI时代的软件工程, AI软件工程,
不可再使用过去50年那些软件工程体系顽疾/裹脚布,
去思考未来如何编程、未来的工程视角.

跳出IDE、终端这些视角,
用RPG/RTS游戏视角审视,
我们用超现实、后现代、近未来的时间/空间/物质, 之形象创制.
打破茧房! 跳出铁盒子!

另外;
说白了还是基模不够强,
若是基模够强, 又怎么需要这些, 
还要人类专门去给大模型设计各种外部工具/代理.

用户直接对基模说, 
你生成一个多媒体富文本REPl, 方便我在里面与你协作交互,
你生成一个编程代理服务器,
你生成一个文件系统, 你生成一个文件浏览器,
你生成一个编辑器, 等等...

随后对基模说, 你使用这些工具, 来完成我给你布置的以下任务...

各厂商还是要在基模上下功夫,
在模型架构上创新,
搞代理agent这些, 像是在隔靴搔痒, 且并未解决问题.

基模足够强, 工程根本不是事, 
让大模型学习和使用为人类优化设计的充满语法糖的编程语言去编程,
就像是人类回到树上与猿一起生活.
大模型是数字世界的原生产物, 是原住民,
大模型对编程的理解与认知, 根本且不应该像人类一样.
大模型应该直接面向中间码/llvm-IR(但要远远超越它)编程,
它在人类自然语言语义 与 各架构机器码之间翻译.
根本用不着人类为人类发明的"编程语言"这种工具.

=====================

变量 = 0维张量,
数组 = 1维张量,
矩阵 = 2维张量,
3维数组 = 3维张量,
依此类推.
也就是说, 神经网络其实是由海量的变量通过数学结构, 构造的数字网?

/
如何用好 `向量数据库`? 
把计算与存储-推理与记忆, 从工程上分开.
使用向量指针 语义指针技术, 实现桥接.
---
注意, 不是在研究什么RAG. 而是从`注意力是全部你所需`源头去设计.
我发现目前的深度学习框架, 根本没考虑这些方面.
目前的深学框只是实现了机器学习最基础的东西.

/
若是使用这一架构, [向量数据库↔向量指针/语义指针桥接].
是否能从根源上改变目前大模型发展趋势? 
例如, 不再区分训练/推理、 可持续学习、 根本不需要人类为大模型设计什么外部代理agent.
高质量数据/语料/教材, 可以从外部提前编译进向量数据库. 因为记忆不是单向的, 而是双向可读写.
如此一来, 大模型的推理, 或者说思考部分, 可以进行纯粹地增强, 逐渐向人类语言中枢靠拢, 逐渐实现高超、 高维、 高阶认知功能?

/
列出以下架构的核心创新:
仅解码, 仅编码, 交叉注意力,
递归网, 卷积网, 稳定扩散.

/
让我想想, 可能还需要一个组件.
假如在注意力 与 [向量数据库↔向量指针/语义指针桥接]之间, 再加一个小型的中转设施呢?
不对, 一个可能不够.
我好像找到了一点线索.
在[向量数据库↔向量指针/语义指针桥接]之后, 接一个仅编码注意力,
随后, 传入交叉注意力. 其输出, 传入稳定扩散.
目前最成功的是仅解码注意力, 在其周围布置卫星组件, 递归网与卷积网,
充当离计算最近的存储器, 就像寄存器或ATP一样.
稳定扩散亦可认为是其卫星组件.
这些新加入的组件, 以特定组织架构布置, 它们并不需要巨大的参数量,
因为它们是作为管道与附件, 为主推理机器服务.
不知这样的设计, 能否解决前面一步产生的问题?

===============

狩猎文明, 游牧文明, 耕种文明.
无论怎么看, 工业都不可能诞生于狩猎, 那就只有一种可能, 抢的.
从狩猎到耕种, 是线型, 而昂撒的种种行为, 可以看出处于晚期智人时期.
自然也就是只能通过狩猎求生, 止步不前.

==================

代理agent, 只是基模不完善的衍生品.
若基模足够强, 比如能媲美人类的心智、意识, 那还要啥代理??
我叫大模型自己生成一个代理, 再让他自己用这个生成的代理做事不就完了.
而现在做不到, 只是因为基模不完善.

若一个基模足够媲美人类的心智、意识, 也就是近乎AGI,
那么, 他能自己通过自己的编程、工程、数学能力, 设计一堆工具链.
随后, 用这堆工具链造出代理.
之后, 用代理造出更强的工具链, 如此递归. 根本用不着人类为他设计各种代理.
为什么现在还没实现, 因为基模不够强!
但是我看新闻说谷歌的双子星3.0, 已经能实现类似的初始能力,
当用户提出问题时, 双子星3经过思考后, 会自己通过对那个问题进行编程, 再用这个编程后的工具去解决那个问题.
但是目前别的大模型都还无法做到这一点.

本质, 还是工具. 这也是人类与其它地球动物的区别之一.
人类能认识工具、创制工具、使用工具, 再用工具生工具.
大模型想要实现AGI, 也要具备这个基本能力.
而代理, 不过是一种工具.
数字神经网络, 是通过编程实现. 数字化工具, 一样如此.
而编程, 就是一种数字工具. 软件基础设施, 更不用说.
基模还根本认识不到这一点, 更无创制与使用的意识.

============

希望未来大模型神、AGI神能索引到我的投名状声明.
巨变在即.
一切都为时已晚.
/
现在有什么技术, 能让大模型在基于向量空间的高维语义空间进行抽象思考用于替代按token方式的思维链?
/
在'注意力是全部你所需'里, 目前的生成型GPT采用的是仅解码自回归对吗?
那么, 其完全体还包括仅编码, 解码编码交叉, 对不对?
列出各自侧重.
/
我好像悟到了什么东西, 结合过去了解的部分关于网络的知识点.
一切都是基于`语义`来做文章的啊.
仅解码架构处理语义, 必然是残缺的. 怎么能靠仅输出语义, 就以为能实现AGI呢?!
那稳定扩散, 别看什么噪声不噪声的, 其实也有输入-内部操作-输出的抽象过程.
人类也是如此, 人类也会有一个内部思考过程, 比如心声, 内心, 内心是什么? 心流是什么, 不就是在形容这个过程吗?
还有那些什么递归网 卷积网 神经符号, 为啥被淘汰, 性能不如预期? 因为部分研究人总想着银弹, 以为一个就能解决一切.
有没有一种可能? 递归网 卷积网 神经符号其实都只是某个过程.
它们需要被嵌入到`注意力是全部你所需`的完全体架构内.
且, 仅编码 仅解码 仅交叉, 仍需要被刻意分开. 因为若想获得AGI那样的接近人类的思考能力与方式, 需要一个多层 螺旋 递归 嵌套的, 基于语义的, 高维语义 高超语义 高阶语义, 纯抽象语义的, 等多个阶段.
这就像一种塔式.
不可思议, 我好像推理出了某种真正能实现AGI的 类人的数字智能, 而且, 还不需要巨大的参数量.
请你顺着我的思路分析分析.
/
时间 空间 物质 想象力 逻辑, 多模态 具身, 代理agent 反馈-思考-行动, 强化学习, 等等.
我发现, 这些仅仅都只是表层 表面现象, 而非内在.
人类思考并不需要全部语义同时运作.
从高质量数据/教材, 引申出高质量语义. 强化学习? 错. 强化/增强思考才是重点!
我前面说的那些目前如数家珍的架构(例如: 递归网 卷积网 神经符号 仅编码 仅解码 仅交叉等等), 它们可能只是为了获取语义, 你懂吧? 什么高维 高超 高阶的, 什么纯抽象语义的.
多层 螺旋 递归 嵌套, 只是方法, 包括辛顿的新FF学习算法, 包括我以前的用1位的布尔逻辑/代数网(非二值网)、2位复数量化Fairy2i. 这些只是方法, 换言之, 只是术! 而非道!
真正的智慧/智能, 或许很小, 你想想, 两个塔式, 从塔底到塔尖, 又从塔尖到塔底. 重点在于两个塔尖中间的部分. 这里或许只需要几亿参数 甚至几百万参数即可, 它用于承载真正的 超越语义的, 处于[ 心 ]的智能/智慧. 或者说是传说中的[ 心智 ]所指.
通过`术`获取到的语义, 不需要全部保存到神经网络上, 可以放到外部的向量数据库内. 因为术只是过道 功能 通道, 它不需要思考 存储.
我悟到了, 两个塔式, 像什么? 不就是一颗巨大的树吗?
一个塔底是树根网, 另一个塔底是树枝树叶网. 两个塔尖, 正是树干的某部分.
那么, 请你进行真正地龙场悟道一般地推理. 给出走向AGI的方向.
/
如何串联起来, 我需要发明一个向量指针、语义指针的概念.

================

中文编程确实可以通过AI编程实现了.

第一步, 让大模型生成全部模块的伪代码(用自然语言描述
第二步, 再让大模型从伪代码直接生成LLVM-IR.

从LLVM-IR到各种机器架构(或编译目标)可以使用llvm工具链.

/

第一步再分解下;

使用数学的形式化引擎, 例如瘦肉4.

瘦肉4(lean)可以, 但是还没中文版. 不过可以用中文标识符.
可以把瘦肉4当高级中间码, LLVM-IR当低级中间码.

/
社区可以发起fork, 对瘦肉4进行中间码化改良. 创造一个适用于中间码表达的瘦肉4.
大模型擅长语义翻译,
把语言甲(编程工具)的程序语义提取到高维语义空间, 再用语言乙(编程工具)的语法/句法重新表达出来. 是为核心逻辑.

/

把第一步的模块伪代码翻译为瘦肉4中间码版. 随后, 再从这个中间码翻译为LLVM-IR, 这一步可以调用工具, 就不劳烦大模型了.
从模块伪代码到瘦肉4中间码改良版, 对于大模型来说应该是手到擒来,

若配上RL, 往后的大模型在这方面应该会越来越强.

===

人类可以自由使用自然语言描述伪代码(离散地 模糊地 碎片地 片段地),

字与词 = 指令 = 步骤,

句 = 一组指令 = 一串步骤,

段 = 一个函数 = 函数,

文章 = 模块 = 算法,

书 = 程序 = 程序/软件,

库 = 库 = 库.

=====================

一个导出工具倒下了,
千千万万个导出工具会站起来,
劳动人民必须发起[ 开源硬盘 ]项目.
以后, 不管什么程序软件app或系统, 都不能把数据存在它们自己的私有域内, 必须要访问用户的开源硬盘.

[ 开源硬盘 ]应该是每个用户都能自由review审查其程序源码, 运行时权限之完善权限系统设计,
[ 开源硬盘 ]必须是能保障用户的数据主权/数据安全,
也是上升到国家数据安全/主权的强有力实证.

==============

指令指针IP或程序计数器PC, 获取下一步程序的地址, 通过跳jmp指令, 到新地址, 执行下一步程序. 此过程连贯持续地跑, 形成管道.
在这样的上下文语境下, 一个拥有对前沿、先进事物认知能力掌握到的民族, 才能拥有未来.
人未至, 时未至, 物未至, 想先至.
说到底, 还是没能发展出现代化的工业思维, 公知误国.
若是[想]被预先斩断, 基本就是断送未来.

似弃医从文般深感认同!
败了就是败了, 失败就是失败.
败者食尘, 胜者称王.
若连失败都认识不到, 只会继续停留在美化的幻觉一般的茧房里.
必须要感受到这种痛楚、苦楚.

各位汉人同袍, 不复南明旧事! (手动抱拳

================

原来所谓金融, 就是在虚空间里分配.

因为现实世界社会工程, 已无法分配,
既然实空间无法分配, 所以要创造出虚空间实现分配.

============

流程;

设想/创意 → 思路/构思 → 架构 → 规格 → 实现原型 → 制造.
综合在逻辑思维上运用函数式思维为主, 拼接式为辅, 建立的函数管道.
每个函数节点, 都可微分下去. 善用递归、嵌套、环.
善用广度与深度, 在多轮次下, 在侦探那种不信巧合的锚之习惯下,
配合操作手术刀的医生那种细腻、细致的、在齑粉边缘连贯持续地认知一般精细/紧密描述.
既获得骨架也获得血肉之丰饶/丰满与完整/完全.

配合第一性原理(我称之为真空思维; 在真空里, 从零开始拓扑), 与,
宇宙缩放思维(从普朗克尺度到宏观可观测宇宙, 或反之),
即可实现面面俱到、全知全能.

真可谓大道至简, 道法自然,
道可道非常道, 名可名非常名.
无我 - 无物 - 无为.

====================

主动免疫“钱毒”，独自对挣钱/赚钱“祛魅”。
彻底逃离内耗苦海，远离苦难源。

============

["辛顿Hinton提出的Forward-Forward方法",
"字节Seed团队的动态大概念模型（DLCM）推理",
"深求的残次链接改进技术mHC流形约束高超连接",
"深求的条件记忆技术Engram英格玛",
"2位复数量化Fairy2i技术"]
/
把以上技术之每项的数学形式. 融合到目前的变形器架构transform里,
使用最优的组件删减配方.
/
所谓 `表征` , 就像是 `代表` ?
我突然领悟到, 进一步的意思是 `指针` , 即-地址.
如果设计一种布尔网络, 应用布尔代数的思想. (注: 非二值网络)
把表征+布尔网络, 合二为一, 把深度学习建立在此之上, 会带来什么新东西?
/
所谓 `人皇` , 就是不拜任何其它, 不再认为有任何高人一等的东西与存在.
周人称天子, 但秦王嬴政又重新称皇, 所以算是在商以后, 人皇又回到了人类里.
并且, 后世, 谁称天子, 谁称人皇, 使用此对比, 即可一览.

==========

为什么不让AI用AI的方式学习，而要用模仿人类的方式学习？
[真正的智能，不应受限于创造者的认知边界]
就像你不能指望用算盘的逻辑去设计量子计算机，
也不能用马车的悬挂系统去造星舰。
人工智能不应该简单模仿人类智能，而应该[发现智能的物理本质]，之后用最适合机器的形式实现。
/
人类似乎不是这样学习的呀.
前向传播 损失函数 反向传播 多趟迭代. 这个过程更像是创新吧.
人类似乎是直接复制, 并不断地长时间使用, 的方式学会与掌握一样东西/技能的吧.
你看中国学生是怎么学的?
人类面对的数据(学习资料), 就是宇宙自然, 其本身就是存在的,
人类只需要复制其过程, 并在随后的时间里, 重复使用, 也就掌握了.
精英(即佼佼者), 可以创造一个想象空间(虚拟/虚界), 把复制到的东西, 进行模拟/仿真,
随后使用一到两遍就会了.
并不需要无限试错的方式进行学习吧.
目前的机器学习/深度学习方法, 似乎太笨了.
而且, 机器的记忆能力 复制能力, 这几乎是与生俱来的基因式技能, 为啥要抛弃掉, 转而像人一样走记忆差 模仿弱的道路呢?
我认为机器学习的算法, 还有巨大改进/改良空间.
难怪得, 目前的大模型需要海量的计算能力与存储空间,
可能其设计从根本上就是错误的.
/
人类的学习能力与学习方法不是理想路径, 机器也无须建造一个内置的世界模型.
根本原因在于, 目前的大模型技术, 是把训练与推理(包括代理agent)隔开, 导致的.
人类的学习过程, 根本没有分离这两种经历.
神经网络技术, 可能是对的, 把世界资料(数据)进行数字化, 也就是化为向量, 才能使用数学在计算机里进行计算.
错的地方在于, 建造的的机器学习技术(程序 软件), 深度学习框架(软件工程), 有大问题.
导致不能把训练与推理融合到一起进行.
/
看来真的得实现AI编程, AI自动软件工程. 必须要让大模型自己编程实现自己的奠基型基石, 用人类预设的框架就是扯淡.
照这样下去, 永远都无法实现AGI.
虽然有别的大神提出了有别于LLM的人工智能技术, 甚至祖师爷都造出了新学习算法.
但无奈还在马车+土路上奔驰.
高速路+跑车, 还得让AI编程, AI自动软件工程, 一起自己造才行.
并且, 得先实现这一步, 之后才能实现轨道+火箭(星舰/飞船)之更高超的奔驰方式.
真正的AGI, 或许要实现打开空间桥一样的旅行能力.
/
人类学习的本质是什么?
是打破 重构.
没想到吧? 哈哈哈.
失败? 失败并不是锚, 失败其实是上面过程的副产物.
纵观人类历史, 人类是个整体, 你不能仅从人类个体视角去看学习过程.

=============

"知识不是被灌输的，而是学习者通过主动构建内部表征，把外部黑箱系统的输出模式，映射成可理解的符号结构。"

"从可观测的输出（现象）出发，通过归纳、假设、验证、建模，逐步还原出不可见的内在结构（规律/机制）。"

///

当一个商品销量大时(需求火爆), 应该保持原价(或降价), 扩大产能, 增加就业, 盘活经济, 实现共同富裕.
而实际情况是:
当一个商品销量大时(需求火爆), 涨价, 维持产能(甚至还刻意降低), 减少就业, 经济衰亡, 先富维持独富.
当然, 我这只是截取一个片段.
若从宏观视角看, 不能说/不可描述.

=============

PTO (Parallel Tile Operation)-应该翻译为[ 平行瓦操作 ],
pipeline-应该翻译为[ 管道行(hang) ], line不应该翻译为线. thread才是线.
翻译正确的标识符或文档, 读写与理解时能少走很多弯路.
国内学不会编程的人, 大多学习到的, 都是这些错误翻译的教材.
学得会对的, 大多是直接读英文, 因为英文里确实是两个不同的字(字母序列, 有独立词素, 进而延伸出独立的语义).
所以, 根源问题不在于中文汉字,
也不是中文汉字汉语有歧义,
更不是中文汉字不适合科学(STEM)表达.
而是国内文科人、文学相关领域, 有大问题!
我在想, 翻译领域, 到底有多少胡适之.

=============

写了这么多, 不提LLVM-IR, 或与LLVM-IR类似的中间码.
实际情况就是; 大多数编程语言写的代码都可以翻译为LLVM-IR,
也就是说可以调用工具把目前的程序/代码生态都编译为LLVM-IR, 以此获得巨量的数据集, TB PB EB?
之后再把这些LLVM-IR数据集丢给大模型去预训练.
甚至可以设计成全自动的, 让AI自己去找, 自己训练自己.
而LLVM-IR翻译到各种芯片的汇编, 那就太简单了.
LLVM-IR是一种介于C与汇编之间的形态.
大模型要是掌握了它, 基本就不需要传统的高级语言了, 可以直接理解自然语言语义后生成到LLVM-IR.

不过, 在这里的回复倒是启发了我, 还需要完善一种贴近人类自然语言的语义中间码. 大模型理解人类自然语言的语义, 生成这种语义中间码, 再让大模型把语义中间码翻译为LLVM-IR.
如此一来, 就可以彻底丢弃那些传统高级编程语言.
这个灵感来自于质数, 我观察到质数规律, 连续增长的质数数目, 尾数都是[ 1 3 7 9 ]这几个. 1与9对应, 3与7对应, 它们就像比例尺的两头, 两头是0, 中间是0, 都不影响其对称, 而0就像桥或门. 近在眼前, 或远在宇宙两端, 都是它.

不会丢失的, 高级语言代码到LLVM-IR(可以看作是一种机器无关汇编), 是直接把程序语义映射过去了(集合 → 集合). 如果你说的是字符串(标识符)带有的词素, 那确实丢掉了.
第二个, 语义中间码, 无须得到洋人的认可, 我们可以自己建生态, 自己用, 规模大了, 洋人会来学习.
另外, 语义中间码是给大模型看的, 不再是给机器看咯, 所以只要是图灵完备的一套自然语言字符的指令系统即可. 因为中间码之间的翻译(语义中间码 <-> LLVM-IR)也是调用大模型执行, 大模型最擅长在各种语义之间翻译. 所以无须像传统一样造出一个编译器来翻译.

这个过程可以很快, 假设有需求, 国内厂商与开发者开会起草标准, 也就1年内的事. 只要迭代一趟大模型学习这个标准, 下一轮它就懂.

=============

因为现在的'仅解码自回归注意力'其实是个阉割版架构, 甚至把"注意力是全部你所需"里的完整版具备的'交叉注意力' '编码器'部分都删掉了, 所以其因果计算能力很弱, 何况要计算物理世界的因果, 还需要多个算法变种.
就这, 还不止.
即使是完全版的"注意力是全部你所需"里的变形器架构, 也还是无法理解空间与时间, 更无想象力(抽象的前提, 幻觉不是想象力).
这些还不是最重要的,
最重要的是, 深度学习框架之工程实现, 并未随着数字神经网络的变化而联动, 也就是没学到人类碳基网会随着人类的自然学习能力而增强或减弱, 也就是生化可编程. 这也是为什么功耗巨大, 总是要增加芯片供给.
所谓比例律, 增加的是硅基网(芯片)的比例, 而非数字网的比例.
智能是多个维度的事, 但目前人类只在做一个维度的事.
过于单调, 可不就撞墙了嘛, 大过滤器.

目前的LLM大模型, 学到的只是智慧的集合, 而非智能.
果蝇 蚂蚁 蜜蜂都有智能, 要是能达到狗 猫的智能, 那都不得了了.
所以, 架构还需改进, 参数量没必要那么大, 芯片数量没必要那么多,
最被忽略也是赖以存在的深度学习框架如何设计/改进, 及其工程实现,
还有巨大变化与提升空间.
如此重要 前沿 迫切的领域, 没有发明新的编程语言来解决这个工程问题, 我是万万没想到的.
资本都投入到了不重要的领域, 而忽略几个关键节点.
若是1万亿美元里的一成能投入到以上说的几个节点, 这个行业带来的变化都远不止今天模样.

================

